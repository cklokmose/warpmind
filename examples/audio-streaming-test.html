<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Audio Streaming Test</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 1000px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f5f5f5;
        }
        .container {
            background: white;
            padding: 30px;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            margin-bottom: 20px;
        }
        .test-section {
            border: 1px solid #ddd;
            margin: 10px 0;
            padding: 15px;
            border-radius: 5px;
        }
        h2 { color: #333; margin-top: 0; }
        button {
            background-color: #007cba;
            color: white;
            padding: 10px 15px;
            margin: 5px;
            border: none;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
        }
        button:hover { background-color: #005a87; }
        button:disabled {
            background-color: #ccc;
            cursor: not-allowed;
        }
        .recording {
            background-color: #dc3545 !important;
            animation: pulse 1s infinite;
        }
        @keyframes pulse {
            0% { opacity: 1; }
            50% { opacity: 0.7; }
            100% { opacity: 1; }
        }
        textarea, input {
            width: 100%;
            padding: 10px;
            margin: 5px 0;
            border: 1px solid #ddd;
            border-radius: 4px;
        }
        #output {
            background: #f8f9fa;
            padding: 15px;
            margin: 10px 0;
            border-radius: 5px;
            max-height: 400px;
            overflow-y: auto;
            font-family: monospace;
            white-space: pre-wrap;
        }
        .progress {
            background: #e9ecef;
            border-radius: 4px;
            overflow: hidden;
            margin: 10px 0;
        }
        .progress-bar {
            background: #007cba;
            height: 20px;
            width: 0%;
            transition: width 0.3s ease;
            color: white;
            text-align: center;
            line-height: 20px;
            font-size: 12px;
        }
        .chunk-indicator {
            display: inline-block;
            width: 10px;
            height: 10px;
            background: #28a745;
            margin: 2px;
            border-radius: 2px;
        }
        .config-section {
            background-color: #f8f9fa;
            border: 2px solid #007cba;
            margin-bottom: 20px;
        }
    </style>
</head>
<body>
    <h1>üéµ Audio Streaming Test Suite</h1>
    <p>Testing textToSpeech and speechToText streaming capabilities</p>

    <!-- Configuration Section -->
    <div class="container config-section">
        <h2>‚öôÔ∏è Configuration</h2>
        <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 15px;">
            <div>
                <label for="serverURL">Server URL:</label>
                <input type="text" id="serverURL" value="http://localhost:8080" placeholder="http://localhost:8080">
            </div>
            <div>
                <label for="apiKey">API Key:</label>
                <input type="password" id="apiKey" value="test-api-key-123" placeholder="Enter your key">
            </div>
        </div>
        <button onclick="updateConfig()">Update Configuration</button>
    </div>

    <div id="output">Ready for testing...</div>

    <!-- Text-to-Speech Tests -->
    <div class="container">
        <div class="test-section">
            <h2>üîä Text-to-Speech Streaming</h2>
            <div>
                <label for="ttsText">Text to speak:</label>
                <textarea id="ttsText" rows="3">This is a test of the text-to-speech streaming functionality. Let's see how well it works with real-time audio chunks!</textarea>
            </div>
            <div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 10px; margin: 10px 0;">
                <div>
                    <label for="voice">Voice:</label>
                    <select id="voice">
                        <option value="alloy">Alloy</option>
                        <option value="echo">Echo</option>
                        <option value="fable">Fable</option>
                        <option value="onyx">Onyx</option>
                        <option value="nova">Nova</option>
                        <option value="shimmer">Shimmer</option>
                    </select>
                </div>
                <div>
                    <label for="format">Format:</label>
                    <select id="format">
                        <option value="mp3">MP3</option>
                        <option value="opus">Opus</option>
                        <option value="aac">AAC</option>
                        <option value="flac">FLAC</option>
                    </select>
                </div>
                <div>
                    <label for="speed">Speed:</label>
                    <input type="range" id="speed" min="0.25" max="4.0" step="0.25" value="1.0">
                    <span id="speedValue">1.0x</span>
                </div>
            </div>
            <button onclick="testTTSStandard()">Test Standard TTS</button>
            <button onclick="testTTSStreaming()" id="streamTTSBtn">Test Streaming TTS</button>
            <div id="ttsProgress" class="progress" style="display: none;">
                <div id="ttsProgressBar" class="progress-bar">0%</div>
            </div>
            <div id="ttsChunks" style="margin: 10px 0;"></div>
        </div>
    </div>

    <!-- Speech-to-Text Tests -->
    <div class="container">
        <div class="test-section">
            <h2>üé§ Speech-to-Text Streaming</h2>
            <div>
                <input type="file" id="audioFile" accept="audio/*">
                <button onclick="testSTTStandard()">Test Standard STT</button>
                <button onclick="testSTTStreaming()" id="streamSTTBtn">Test Streaming STT</button>
            </div>
            <div style="margin: 10px 0;">
                <button onclick="startRecording()" id="recordBtn">üé§ Start Recording</button>
                <button onclick="stopRecording()" id="stopBtn" disabled>‚èπÔ∏è Stop Recording</button>
            </div>
            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 10px; margin: 10px 0;">
                <div>
                    <label for="language">Language (optional):</label>
                    <input type="text" id="language" placeholder="en, es, fr, etc.">
                </div>
                <div>
                    <label for="prompt">Prompt (optional):</label>
                    <input type="text" id="prompt" placeholder="Context or expected words">
                </div>
            </div>
            <div id="sttProgress" class="progress" style="display: none;">
                <div id="sttProgressBar" class="progress-bar">Processing...</div>
            </div>
            <div id="partialTranscript" style="background: #e3f2fd; padding: 10px; margin: 10px 0; border-radius: 4px; min-height: 40px; font-style: italic;">
                Partial transcripts will appear here during streaming...
            </div>
        </div>
    </div>

    <!-- Combined Voice Chat Test -->
    <div class="container">
        <div class="test-section">
            <h2>üí¨ Combined Voice Chat Test</h2>
            <p>Tests both streaming STT and TTS together</p>
            <button onclick="testVoiceChat()" id="voiceChatBtn">Start Voice Chat Session</button>
            <button onclick="stopVoiceChat()" id="stopVoiceChatBtn" disabled>Stop Voice Chat</button>
            <div id="voiceChatControls" style="margin: 10px 0; display: none;">
                <button onclick="startRecording()" id="startRecordingBtn" disabled>üé§ Start Recording</button>
                <button onclick="stopRecordingAndRespond()" id="stopRecordingBtn" disabled>‚èπÔ∏è Stop Recording & Get Response</button>
            </div>
            <div id="voiceChatStatus" style="margin: 10px 0; padding: 10px; background: #f8f9fa; border-radius: 4px;">
                Ready to start voice chat...
            </div>
            <div id="conversationHistory" style="margin: 10px 0; padding: 10px; background: #f0f0f0; border-radius: 4px; max-height: 300px; overflow-y: auto; display: none;">
                <h4>Conversation:</h4>
                <div id="conversationContent"></div>
            </div>
        </div>
    </div>

    <!-- Performance Metrics -->
    <div class="container">
        <div class="test-section">
            <h2>üìä Performance Metrics</h2>
            <div id="metrics" style="font-family: monospace; background: #f8f9fa; padding: 10px; border-radius: 4px;">
                No tests run yet...
            </div>
        </div>
    </div>

    <script src="../dist/warpmind.js"></script>
    <script>
        let wm = null;
        let isRecording = false;
        let mediaRecorder = null;
        let audioChunks = [];
        let currentVoiceChat = null;
        let metrics = {
            ttsChunks: 0,
            sttPartials: 0,
            startTime: 0,
            endTime: 0
        };

        function log(message, type = 'info') {
            const timestamp = new Date().toLocaleTimeString();
            const prefix = type === 'error' ? '‚ùå' : type === 'success' ? '‚úÖ' : type === 'warning' ? '‚ö†Ô∏è' : '‚ÑπÔ∏è';
            const output = document.getElementById('output');
            output.textContent += `[${timestamp}] ${prefix} ${message}\n`;
            output.scrollTop = output.scrollHeight;
        }

        function updateConfig() {
            const serverURL = document.getElementById('serverURL').value.trim();
            const apiKey = document.getElementById('apiKey').value.trim();
            
            if (!serverURL || !apiKey) {
                log('Please fill in both server URL and API key', 'error');
                return;
            }

            const baseURL = serverURL.endsWith('/v1') ? serverURL : serverURL.replace(/\/$/, '') + '/v1';
            
            wm = new WarpMind({
                apiKey: apiKey,
                baseURL: baseURL
            });
            
            log('Configuration updated successfully', 'success');
        }

        function updateMetrics() {
            const metricsDiv = document.getElementById('metrics');
            const duration = metrics.endTime - metrics.startTime;
            
            metricsDiv.textContent = `
Last Test Metrics:
- Duration: ${duration}ms
- TTS Chunks received: ${metrics.ttsChunks}
- STT Partials received: ${metrics.sttPartials}
- Average chunk interval: ${metrics.ttsChunks > 0 ? Math.round(duration / metrics.ttsChunks) : 0}ms
            `.trim();
        }

        // Update speed display
        document.getElementById('speed').addEventListener('input', function() {
            document.getElementById('speedValue').textContent = this.value + 'x';
        });

        // TTS Tests
        async function testTTSStandard() {
            if (!wm) {
                log('Please configure the API first', 'error');
                return;
            }

            const text = document.getElementById('ttsText').value;
            const voice = document.getElementById('voice').value;
            const format = document.getElementById('format').value;
            const speed = parseFloat(document.getElementById('speed').value);

            metrics = { ...metrics, ttsChunks: 0, startTime: Date.now() };
            
            try {
                log('Starting standard TTS test...', 'info');
                const audioBlob = await wm.textToSpeech(text, {
                    voice: voice,
                    format: format,
                    speed: speed
                });

                metrics.endTime = Date.now();
                
                log(`TTS completed. Audio size: ${audioBlob.size} bytes`, 'success');
                
                // Play the audio
                const audio = new Audio(URL.createObjectURL(audioBlob));
                audio.play();
                
                updateMetrics();
            } catch (error) {
                log(`TTS failed: ${error.message}`, 'error');
            }
        }

        async function testTTSStreaming() {
            if (!wm) {
                log('Please configure the API first', 'error');
                return;
            }

            const text = document.getElementById('ttsText').value;
            const voice = document.getElementById('voice').value;
            const format = document.getElementById('format').value;
            const speed = parseFloat(document.getElementById('speed').value);

            const btn = document.getElementById('streamTTSBtn');
            const progress = document.getElementById('ttsProgress');
            const progressBar = document.getElementById('ttsProgressBar');
            const chunksDiv = document.getElementById('ttsChunks');
            
            btn.disabled = true;
            progress.style.display = 'block';
            chunksDiv.innerHTML = '';
            
            metrics = { ttsChunks: 0, sttPartials: 0, startTime: Date.now() };
            
            try {
                log('Starting streaming TTS test...', 'info');
                
                const audioBlob = await wm.textToSpeech(text, {
                    voice: voice,
                    format: format,
                    speed: speed,
                    stream: true,
                    onChunk: (chunk) => {
                        metrics.ttsChunks++;
                        progressBar.style.width = `${Math.min(100, metrics.ttsChunks * 10)}%`;
                        progressBar.textContent = `Chunk ${metrics.ttsChunks}`;
                        
                        // Visual indicator
                        const indicator = document.createElement('span');
                        indicator.className = 'chunk-indicator';
                        indicator.title = `Chunk ${metrics.ttsChunks}: ${chunk.byteLength} bytes`;
                        chunksDiv.appendChild(indicator);
                        
                        log(`Received audio chunk ${metrics.ttsChunks}: ${chunk.byteLength} bytes`);
                    }
                });

                metrics.endTime = Date.now();
                
                log(`Streaming TTS completed. Final audio size: ${audioBlob.size} bytes`, 'success');
                
                // Play the combined audio
                const audio = new Audio(URL.createObjectURL(audioBlob));
                audio.play();
                
                updateMetrics();
            } catch (error) {
                log(`Streaming TTS failed: ${error.message}`, 'error');
            } finally {
                btn.disabled = false;
                progress.style.display = 'none';
            }
        }

        // STT Tests
        async function testSTTStandard() {
            if (!wm) {
                log('Please configure the API first', 'error');
                return;
            }

            const fileInput = document.getElementById('audioFile');
            if (!fileInput.files[0]) {
                log('Please select an audio file first', 'error');
                return;
            }

            const language = document.getElementById('language').value;
            const prompt = document.getElementById('prompt').value;

            metrics = { ...metrics, sttPartials: 0, startTime: Date.now() };
            
            try {
                log('Starting standard STT test...', 'info');
                const transcript = await wm.speechToText(fileInput.files[0], {
                    language: language || undefined,
                    prompt: prompt || undefined
                });

                metrics.endTime = Date.now();
                
                log(`STT completed. Transcript: "${transcript}"`, 'success');
                updateMetrics();
            } catch (error) {
                log(`STT failed: ${error.message}`, 'error');
            }
        }

        async function testSTTStreaming() {
            if (!wm) {
                log('Please configure the API first', 'error');
                return;
            }

            const fileInput = document.getElementById('audioFile');
            if (!fileInput.files[0]) {
                log('Please select an audio file first', 'error');
                return;
            }

            const language = document.getElementById('language').value;
            const prompt = document.getElementById('prompt').value;
            const btn = document.getElementById('streamSTTBtn');
            const progress = document.getElementById('sttProgress');
            const partialDiv = document.getElementById('partialTranscript');
            
            btn.disabled = true;
            progress.style.display = 'block';
            partialDiv.textContent = 'Processing...';
            
            metrics = { ttsChunks: 0, sttPartials: 0, startTime: Date.now() };
            
            try {
                log('Starting streaming STT test...', 'info');
                
                const transcript = await wm.speechToText(fileInput.files[0], {
                    language: language || undefined,
                    prompt: prompt || undefined,
                    stream: true,
                    onPartial: (partialText) => {
                        metrics.sttPartials++;
                        partialDiv.textContent = partialText;
                        log(`Partial transcript ${metrics.sttPartials}: "${partialText}"`);
                    }
                });

                metrics.endTime = Date.now();
                
                log(`Streaming STT completed. Final transcript: "${transcript}"`, 'success');
                updateMetrics();
            } catch (error) {
                log(`Streaming STT failed: ${error.message}`, 'error');
            } finally {
                btn.disabled = false;
                progress.style.display = 'none';
            }
        }

        // Recording functionality
        async function startRecording() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                mediaRecorder = new MediaRecorder(stream);
                audioChunks = [];

                mediaRecorder.ondataavailable = event => {
                    audioChunks.push(event.data);
                };

                mediaRecorder.onstop = async () => {
                    const audioBlob = new Blob(audioChunks, { type: 'audio/wav' });
                    log(`Recording stopped. Size: ${audioBlob.size} bytes`, 'success');
                    
                    // Auto-test with the recorded audio
                    if (wm) {
                        try {
                            const transcript = await wm.speechToText(audioBlob, {
                                stream: true,
                                onPartial: (text) => {
                                    document.getElementById('partialTranscript').textContent = text;
                                    log(`Live transcript: "${text}"`);
                                }
                            });
                            log(`Final recording transcript: "${transcript}"`, 'success');
                        } catch (error) {
                            log(`Recording transcription failed: ${error.message}`, 'error');
                        }
                    }
                };

                mediaRecorder.start();
                isRecording = true;
                
                document.getElementById('recordBtn').disabled = true;
                document.getElementById('recordBtn').classList.add('recording');
                document.getElementById('stopBtn').disabled = false;
                
                log('Recording started...', 'info');
            } catch (error) {
                log(`Recording failed: ${error.message}`, 'error');
            }
        }

        function stopRecording() {
            if (mediaRecorder && isRecording) {
                mediaRecorder.stop();
                isRecording = false;
                
                document.getElementById('recordBtn').disabled = false;
                document.getElementById('recordBtn').classList.remove('recording');
                document.getElementById('stopBtn').disabled = true;
                
                // Stop all tracks
                mediaRecorder.stream.getTracks().forEach(track => track.stop());
            }
        }

        // Voice Chat Test
        async function testVoiceChat() {
            if (!wm) {
                log('Please configure the API first', 'error');
                return;
            }

            const btn = document.getElementById('voiceChatBtn');
            const stopBtn = document.getElementById('stopVoiceChatBtn');
            const status = document.getElementById('voiceChatStatus');
            
            try {
                btn.disabled = true;
                stopBtn.disabled = false;
                status.textContent = 'Initializing voice chat...';
                
                currentVoiceChat = wm.createVoiceChat(
                    'You are a helpful assistant. Keep responses brief and conversational.',
                    {
                        stt: { 
                            stream: true,
                            onPartial: (text) => {
                                status.textContent = `Listening: ${text}`;
                            }
                        },
                        tts: { 
                            stream: true, 
                            voice: 'nova',
                            onChunk: (chunk) => {
                                log(`TTS chunk: ${chunk.byteLength} bytes`);
                            }
                        }
                    }
                );
                
                // Show recording controls and conversation history
                document.getElementById('voiceChatControls').style.display = 'block';
                document.getElementById('conversationHistory').style.display = 'block';
                document.getElementById('startRecordingBtn').disabled = false;
                
                status.textContent = 'Voice chat ready. Click "Start Recording" to begin conversation.';
                log('Voice chat initialized with streaming support', 'success');
                
            } catch (error) {
                log(`Voice chat failed: ${error.message}`, 'error');
                btn.disabled = false;
                stopBtn.disabled = true;
                status.textContent = 'Voice chat failed to initialize';
            }
        }

        function stopVoiceChat() {
            if (currentVoiceChat) {
                // Voice chat doesn't have a specific stop method in the current implementation
                // but we can reset the UI
                currentVoiceChat = null;
                
                document.getElementById('voiceChatBtn').disabled = false;
                document.getElementById('stopVoiceChatBtn').disabled = true;
                document.getElementById('voiceChatStatus').textContent = 'Voice chat stopped';
                
                // Hide recording controls and conversation
                document.getElementById('voiceChatControls').style.display = 'none';
                document.getElementById('conversationHistory').style.display = 'none';
                
                log('Voice chat session ended', 'info');
            }
        }

        async function startRecording() {
            if (!currentVoiceChat) {
                log('No active voice chat session', 'error');
                return;
            }

            try {
                const startBtn = document.getElementById('startRecordingBtn');
                const stopBtn = document.getElementById('stopRecordingBtn');
                const status = document.getElementById('voiceChatStatus');

                startBtn.disabled = true;
                stopBtn.disabled = false;
                status.textContent = 'Recording... Speak now!';

                const success = await currentVoiceChat.startRecording();
                if (success) {
                    log('Recording started successfully', 'success');
                } else {
                    throw new Error('Failed to start recording');
                }
            } catch (error) {
                log(`Recording failed: ${error.message}`, 'error');
                document.getElementById('startRecordingBtn').disabled = false;
                document.getElementById('stopRecordingBtn').disabled = true;
                document.getElementById('voiceChatStatus').textContent = 'Recording failed. Try again.';
            }
        }

        async function stopRecordingAndRespond() {
            if (!currentVoiceChat) {
                log('No active voice chat session', 'error');
                return;
            }

            try {
                const startBtn = document.getElementById('startRecordingBtn');
                const stopBtn = document.getElementById('stopRecordingBtn');
                const status = document.getElementById('voiceChatStatus');
                const conversationContent = document.getElementById('conversationContent');

                stopBtn.disabled = true;
                status.textContent = 'Processing your message...';

                const result = await currentVoiceChat.stopRecordingAndRespond();
                
                // Add to conversation history
                const userMessage = document.createElement('div');
                userMessage.innerHTML = `<strong>You:</strong> ${result.userMessage}`;
                userMessage.style.marginBottom = '10px';
                userMessage.style.padding = '5px';
                userMessage.style.backgroundColor = '#e3f2fd';
                userMessage.style.borderRadius = '4px';
                
                const aiMessage = document.createElement('div');
                aiMessage.innerHTML = `<strong>AI:</strong> ${result.aiResponse}`;
                aiMessage.style.marginBottom = '10px';
                aiMessage.style.padding = '5px';
                aiMessage.style.backgroundColor = '#f3e5f5';
                aiMessage.style.borderRadius = '4px';

                conversationContent.appendChild(userMessage);
                conversationContent.appendChild(aiMessage);
                conversationContent.scrollTop = conversationContent.scrollHeight;

                // Play the AI response
                if (result.speechBlob) {
                    const audioUrl = URL.createObjectURL(result.speechBlob);
                    const audio = new Audio(audioUrl);
                    audio.play().then(() => {
                        log('AI response played successfully', 'success');
                    }).catch(error => {
                        log(`Audio playback failed: ${error.message}`, 'error');
                    });
                }

                startBtn.disabled = false;
                status.textContent = 'Ready for next recording. Click "Start Recording" to continue.';
                
                log(`Conversation turn completed. User: "${result.userMessage}" | AI: "${result.aiResponse}"`, 'success');

            } catch (error) {
                log(`Voice chat failed: ${error.message}`, 'error');
                document.getElementById('startRecordingBtn').disabled = false;
                document.getElementById('voiceChatStatus').textContent = 'Processing failed. Try recording again.';
            }
        }

        // Initialize with default config
        window.addEventListener('load', () => {
            updateConfig();
            log('Audio streaming test suite loaded', 'success');
        });
    </script>
</body>
</html>
